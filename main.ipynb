{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvS1fOJMsW4S",
        "outputId": "001a4105-07a2-4531-e48b-a55c4b06c67c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/data.zip'\n",
        "target_dir = '/content'\n",
        "\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWgHBr75unKI",
        "outputId": "dad3145c-a243-4629-f984-cdcae3b084c4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Collecting timm==1.0.13\n",
            "  Downloading timm-1.0.13-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm==1.0.13) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==1.0.13) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm==1.0.13) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm==1.0.13) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm==1.0.13) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.13) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.13) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.13) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.13) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==1.0.13) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm==1.0.13) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm==1.0.13) (3.0.3)\n",
            "Downloading timm-1.0.13-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: yacs, timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.22\n",
            "    Uninstalling timm-1.0.22:\n",
            "      Successfully uninstalled timm-1.0.22\n",
            "Successfully installed timm-1.0.13 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm==1.0.13 yacs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lucinnnal/Intelligence_Vision.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2I9pdnxIpLS",
        "outputId": "c76439ad-72cb-4506-815c-011f056d84c3",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intelligence_Vision'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 111 (delta 47), reused 102 (delta 38), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (111/111), 1.41 MiB | 23.72 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_path = '/content/data'\n",
        "destination_path = '/content/Intelligence_Vision/data'\n",
        "\n",
        "if os.path.exists(source_path):\n",
        "    # If destination exists and is not empty, remove it first\n",
        "    if os.path.exists(destination_path):\n",
        "        if os.path.isdir(destination_path) and len(os.listdir(destination_path)) > 0:\n",
        "            print(f\"Destination directory '{destination_path}' exists and is not empty. Removing it.\")\n",
        "            shutil.rmtree(destination_path)\n",
        "        elif os.path.isdir(destination_path) and len(os.listdir(destination_path)) == 0:\n",
        "            print(f\"Destination directory '{destination_path}' exists but is empty. Removing it.\")\n",
        "            os.rmdir(destination_path)\n",
        "        elif os.path.isfile(destination_path):\n",
        "            print(f\"Destination path '{destination_path}' is a file. Removing it.\")\n",
        "            os.remove(destination_path)\n",
        "\n",
        "    # Now, move the source to the destination\n",
        "    shutil.move(source_path, destination_path)\n",
        "    print(f\"Moved '{source_path}' to '{destination_path}'\")\n",
        "else:\n",
        "    print(f\"Source directory '{source_path}' does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-m2wjE_2JJcm",
        "outputId": "f3f0945c-faa8-49bc-e95f-8d86855dd939"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destination directory '/content/Intelligence_Vision/data' exists and is not empty. Removing it.\n",
            "Moved '/content/data' to '/content/Intelligence_Vision/data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_file = '/content/drive/MyDrive/mfm_pretrain_vit_base.pth'\n",
        "destination_dir = '/content/Intelligence_Vision/pretrained'\n",
        "\n",
        "# Create destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "if os.path.exists(source_file):\n",
        "    shutil.move(source_file, destination_dir)\n",
        "    print(f\"Moved '{source_file}' to '{destination_dir}'\")\n",
        "else:\n",
        "    print(f\"Source file '{source_file}' does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iz3mQklZ2xv",
        "outputId": "3c8fc0c7-ef66-47a5-d349-c976080f32c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source file '/content/drive/MyDrive/mfm_pretrain_vit_base.pth' does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Intelligence_Vision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_D0U0UsIylk",
        "outputId": "d66edd41-49b3-4717-c79e-60018db3662a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Intelligence_Vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --config /content/Intelligence_Vision/configs/clip.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBydegWbJwb-",
        "outputId": "5a71d24f-0a6f-43ef-d588-c0f222adb124",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17 12:08:27.769572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765973307.802472     901 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765973307.812687     901 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765973307.837144     901 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765973307.837170     901 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765973307.837177     901 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765973307.837184     901 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 12:08:27.844093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Configuration loaded from /content/Intelligence_Vision/configs/clip.json\n",
            "Using device: cuda\n",
            "Scanning train dataset in ./data/train...\n",
            "  [ADM] found 799 images\n",
            "  [DDPM] found 800 images\n",
            "  [Diff-ProjectedGAN] found 800 images\n",
            "  [Diff-StyleGAN2] found 800 images\n",
            "  [IDDPM] found 800 images\n",
            "  [LDM] found 800 images\n",
            "  [PNDM] found 800 images\n",
            "  [ProGAN] found 800 images\n",
            "  [ProjectedGAN] found 800 images\n",
            "  [StyleGAN] found 800 images\n",
            "Total train images: 7999\n",
            "\n",
            "Scanning val dataset in ./data/val...\n",
            "  [ADM] found 100 images\n",
            "  [DDPM] found 100 images\n",
            "  [Diff-ProjectedGAN] found 100 images\n",
            "  [Diff-StyleGAN2] found 100 images\n",
            "  [IDDPM] found 100 images\n",
            "  [LDM] found 100 images\n",
            "  [PNDM] found 100 images\n",
            "  [ProGAN] found 100 images\n",
            "  [ProjectedGAN] found 100 images\n",
            "  [StyleGAN] found 100 images\n",
            "Total val images: 1000\n",
            "\n",
            "Scanning test dataset in ./data/test...\n",
            "  [ADM] found 101 images\n",
            "  [DDPM] found 100 images\n",
            "  [Diff-ProjectedGAN] found 100 images\n",
            "  [Diff-StyleGAN2] found 100 images\n",
            "  [IDDPM] found 100 images\n",
            "  [LDM] found 100 images\n",
            "  [PNDM] found 100 images\n",
            "  [ProGAN] found 100 images\n",
            "  [ProjectedGAN] found 100 images\n",
            "  [StyleGAN] found 100 images\n",
            "Total test images: 1001\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Loading CLIP model: openai/clip-vit-base-patch32\n",
            "config.json: 4.19kB [00:00, 18.0MB/s]\n",
            "pytorch_model.bin: 100% 605M/605M [00:03<00:00, 161MB/s]\n",
            "model.safetensors:   0% 0.00/605M [00:00<?, ?B/s]Training parameters: Classifier\n",
            "Freezing parameters: CLIP Encoder\n",
            "\n",
            "Optimizer: AdamW, Scheduler: CosineAnnealingLR\n",
            "\n",
            "--- Starting Training ---\n",
            "\n",
            "Epoch 1/20\n",
            "\n",
            "model.safetensors: 100% 605M/605M [00:05<00:00, 112MB/s]\n",
            "\n",
            "Training:   0% 0/250 [00:05<?, ?it/s, loss=2.4386, acc=6.25%]\u001b[A\n",
            "Training:   0% 1/250 [00:05<21:06,  5.09s/it, loss=2.4386, acc=6.25%]\u001b[A\n",
            "Training:   0% 1/250 [00:05<21:06,  5.09s/it, loss=2.3525, acc=7.81%]\u001b[A\n",
            "Training:   1% 2/250 [00:05<08:59,  2.17s/it, loss=2.3525, acc=7.81%]\u001b[A\n",
            "Training:   1% 2/250 [00:05<08:59,  2.17s/it, loss=2.2367, acc=9.38%]\u001b[A\n",
            "Training:   1% 3/250 [00:05<05:05,  1.24s/it, loss=2.2367, acc=9.38%]\u001b[A\n",
            "Training:   1% 3/250 [00:05<05:05,  1.24s/it, loss=2.4239, acc=10.16%]\u001b[A\n",
            "Training:   2% 4/250 [00:05<03:13,  1.27it/s, loss=2.4239, acc=10.16%]\u001b[A\n",
            "Training:   2% 4/250 [00:05<03:13,  1.27it/s, loss=2.4259, acc=10.00%]\u001b[A\n",
            "Training:   2% 4/250 [00:05<03:13,  1.27it/s, loss=2.5273, acc=10.42%]\u001b[A\n",
            "Training:   2% 6/250 [00:05<01:44,  2.35it/s, loss=2.5273, acc=10.42%]\u001b[A\n",
            "Training:   2% 6/250 [00:05<01:44,  2.35it/s, loss=2.5274, acc=8.93%] \u001b[A\n",
            "Training:   2% 6/250 [00:05<01:44,  2.35it/s, loss=2.3592, acc=8.98%]\u001b[A\n",
            "Training:   3% 8/250 [00:05<01:08,  3.52it/s, loss=2.3592, acc=8.98%]\u001b[A\n",
            "Training:   3% 8/250 [00:05<01:08,  3.52it/s, loss=2.3431, acc=10.42%]\u001b[A\n",
            "Training:   4% 9/250 [00:05<00:58,  4.12it/s, loss=2.3431, acc=10.42%]\u001b[A\n",
            "Training:   4% 9/250 [00:06<00:58,  4.12it/s, loss=2.3923, acc=10.62%]\u001b[A\n",
            "Training:   4% 9/250 [00:06<00:58,  4.12it/s, loss=2.3231, acc=11.08%]\u001b[A\n",
            "Training:   4% 11/250 [00:06<00:44,  5.38it/s, loss=2.3231, acc=11.08%]\u001b[A\n",
            "Training:   4% 11/250 [00:06<00:44,  5.38it/s, loss=2.2851, acc=11.72%]\u001b[A\n",
            "Training:   5% 12/250 [00:06<00:39,  5.98it/s, loss=2.2851, acc=11.72%]\u001b[A\n",
            "Training:   5% 12/250 [00:06<00:39,  5.98it/s, loss=2.3819, acc=11.54%]\u001b[A\n",
            "Training:   5% 13/250 [00:06<00:37,  6.35it/s, loss=2.3819, acc=11.54%]\u001b[A\n",
            "Training:   5% 13/250 [00:06<00:37,  6.35it/s, loss=2.2472, acc=12.50%]\u001b[A\n",
            "Training:   5% 13/250 [00:06<00:37,  6.35it/s, loss=2.2459, acc=12.71%]\u001b[A\n",
            "Training:   6% 15/250 [00:06<00:31,  7.50it/s, loss=2.2459, acc=12.71%]\u001b[A\n",
            "Training:   6% 15/250 [00:06<00:31,  7.50it/s, loss=2.3708, acc=13.09%]\u001b[A\n",
            "Training:   6% 15/250 [00:06<00:31,  7.50it/s, loss=2.1538, acc=13.42%]\u001b[A\n",
            "Training:   7% 17/250 [00:06<00:27,  8.36it/s, loss=2.1538, acc=13.42%]\u001b[A\n",
            "Training:   7% 17/250 [00:06<00:27,  8.36it/s, loss=2.2845, acc=13.02%]\u001b[A\n",
            "Training:   7% 18/250 [00:06<00:27,  8.57it/s, loss=2.2845, acc=13.02%]\u001b[A\n",
            "Training:   7% 18/250 [00:07<00:27,  8.57it/s, loss=2.3858, acc=12.50%]\u001b[A\n",
            "Training:   8% 19/250 [00:07<00:27,  8.46it/s, loss=2.3858, acc=12.50%]\u001b[A\n",
            "Training:   8% 19/250 [00:07<00:27,  8.46it/s, loss=2.1812, acc=13.12%]\u001b[A\n",
            "Training:   8% 20/250 [00:07<00:27,  8.41it/s, loss=2.1812, acc=13.12%]\u001b[A\n",
            "Training:   8% 20/250 [00:07<00:27,  8.41it/s, loss=2.2455, acc=13.39%]\u001b[A\n",
            "Training:   8% 21/250 [00:07<00:31,  7.27it/s, loss=2.2455, acc=13.39%]\u001b[A\n",
            "Training:   8% 21/250 [00:07<00:31,  7.27it/s, loss=2.4090, acc=13.21%]\u001b[A\n",
            "Training:   9% 22/250 [00:07<00:29,  7.72it/s, loss=2.4090, acc=13.21%]\u001b[A\n",
            "Training:   9% 22/250 [00:07<00:29,  7.72it/s, loss=2.2511, acc=13.32%]\u001b[A\n",
            "Training:   9% 23/250 [00:07<00:27,  8.13it/s, loss=2.2511, acc=13.32%]\u001b[A\n",
            "Training:   9% 23/250 [00:07<00:27,  8.13it/s, loss=2.2949, acc=13.41%]\u001b[A\n",
            "Training:  10% 24/250 [00:07<00:27,  8.18it/s, loss=2.2949, acc=13.41%]\u001b[A\n",
            "Training:  10% 24/250 [00:07<00:27,  8.18it/s, loss=2.2626, acc=13.50%]\u001b[A\n",
            "Training:  10% 25/250 [00:07<00:26,  8.48it/s, loss=2.2626, acc=13.50%]\u001b[A\n",
            "Training:  10% 25/250 [00:07<00:26,  8.48it/s, loss=2.2063, acc=13.58%]\u001b[A\n",
            "Training:  10% 25/250 [00:07<00:26,  8.48it/s, loss=2.1738, acc=13.77%]\u001b[A\n",
            "Training:  11% 27/250 [00:07<00:26,  8.51it/s, loss=2.1738, acc=13.77%]\u001b[A\n",
            "Training:  11% 27/250 [00:08<00:26,  8.51it/s, loss=2.2413, acc=14.06%]\u001b[A\n",
            "Training:  11% 28/250 [00:08<00:31,  7.12it/s, loss=2.2413, acc=14.06%]\u001b[A\n",
            "Training:  11% 28/250 [00:08<00:31,  7.12it/s, loss=2.2056, acc=14.22%]\u001b[A\n",
            "Training:  12% 29/250 [00:08<00:29,  7.61it/s, loss=2.2056, acc=14.22%]\u001b[A\n",
            "Training:  12% 29/250 [00:08<00:29,  7.61it/s, loss=2.2032, acc=14.38%]\u001b[A\n",
            "Training:  12% 30/250 [00:08<00:27,  8.08it/s, loss=2.2032, acc=14.38%]\u001b[A\n",
            "Training:  12% 30/250 [00:08<00:27,  8.08it/s, loss=2.1507, acc=14.52%]\u001b[A\n",
            "Training:  12% 31/250 [00:08<00:25,  8.47it/s, loss=2.1507, acc=14.52%]\u001b[A\n",
            "Training:  12% 31/250 [00:08<00:25,  8.47it/s, loss=2.1154, acc=14.84%]\u001b[A\n",
            "Training:  13% 32/250 [00:08<00:37,  5.83it/s, loss=2.1154, acc=14.84%]\u001b[A\n",
            "Training:  13% 32/250 [00:08<00:37,  5.83it/s, loss=2.3486, acc=14.96%]\u001b[A\n",
            "Training:  13% 33/250 [00:08<00:32,  6.59it/s, loss=2.3486, acc=14.96%]\u001b[A\n",
            "Training:  13% 33/250 [00:09<00:32,  6.59it/s, loss=2.1295, acc=15.17%]\u001b[A\n",
            "Training:  14% 34/250 [00:09<00:29,  7.21it/s, loss=2.1295, acc=15.17%]\u001b[A\n",
            "Training:  14% 34/250 [00:09<00:29,  7.21it/s, loss=2.0761, acc=15.36%]\u001b[A\n",
            "Training:  14% 35/250 [00:09<00:28,  7.52it/s, loss=2.0761, acc=15.36%]\u001b[A\n",
            "Training:  14% 35/250 [00:09<00:28,  7.52it/s, loss=2.1519, acc=15.54%]\u001b[A\n",
            "Training:  14% 36/250 [00:09<00:37,  5.73it/s, loss=2.1519, acc=15.54%]\u001b[A\n",
            "Training:  14% 36/250 [00:09<00:37,  5.73it/s, loss=2.1227, acc=15.96%]\u001b[A\n",
            "Training:  14% 36/250 [00:09<00:37,  5.73it/s, loss=2.2235, acc=16.04%]\u001b[A\n",
            "Training:  15% 38/250 [00:09<00:29,  7.11it/s, loss=2.2235, acc=16.04%]\u001b[A\n",
            "Training:  15% 38/250 [00:09<00:29,  7.11it/s, loss=2.0414, acc=16.27%]\u001b[A\n",
            "Training:  15% 38/250 [00:09<00:29,  7.11it/s, loss=2.1856, acc=16.41%]\u001b[A\n",
            "Training:  16% 40/250 [00:09<00:29,  7.07it/s, loss=2.1856, acc=16.41%]\u001b[A\n",
            "Training:  16% 40/250 [00:10<00:29,  7.07it/s, loss=1.9217, acc=16.92%]\u001b[A\n",
            "Training:  16% 40/250 [00:10<00:29,  7.07it/s, loss=2.0763, acc=17.26%]\u001b[A\n",
            "Training:  17% 42/250 [00:10<00:26,  7.89it/s, loss=2.0763, acc=17.26%]\u001b[A\n",
            "Training:  17% 42/250 [00:10<00:26,  7.89it/s, loss=1.9414, acc=17.81%]\u001b[A\n",
            "Training:  17% 43/250 [00:10<00:25,  8.12it/s, loss=1.9414, acc=17.81%]\u001b[A\n",
            "Training:  17% 43/250 [00:10<00:25,  8.12it/s, loss=1.8414, acc=18.18%]\u001b[A\n",
            "Training:  18% 44/250 [00:10<00:33,  6.18it/s, loss=1.8414, acc=18.18%]\u001b[A\n",
            "Training:  18% 44/250 [00:10<00:33,  6.18it/s, loss=1.8028, acc=18.96%]\u001b[A\n",
            "Training:  18% 44/250 [00:10<00:33,  6.18it/s, loss=1.9860, acc=19.02%]\u001b[A\n",
            "Training:  18% 46/250 [00:10<00:27,  7.30it/s, loss=1.9860, acc=19.02%]\u001b[A\n",
            "Training:  18% 46/250 [00:10<00:27,  7.30it/s, loss=2.0792, acc=19.02%]\u001b[A\n",
            "Training:  18% 46/250 [00:11<00:27,  7.30it/s, loss=2.0361, acc=19.27%]\u001b[A\n",
            "Training:  19% 48/250 [00:11<00:29,  6.85it/s, loss=2.0361, acc=19.27%]\u001b[A\n",
            "Training:  19% 48/250 [00:11<00:29,  6.85it/s, loss=1.7853, acc=19.83%]\u001b[A\n",
            "Training:  20% 49/250 [00:11<00:27,  7.31it/s, loss=1.7853, acc=19.83%]\u001b[A\n",
            "Training:  20% 49/250 [00:11<00:27,  7.31it/s, loss=1.6784, acc=20.31%]\u001b[A\n",
            "Training:  20% 50/250 [00:11<00:26,  7.61it/s, loss=1.6784, acc=20.31%]\u001b[A\n",
            "Training:  20% 50/250 [00:11<00:26,  7.61it/s, loss=1.8068, acc=20.53%]\u001b[A\n",
            "Training:  20% 50/250 [00:11<00:26,  7.61it/s, loss=1.6766, acc=20.91%]\u001b[A\n",
            "Training:  21% 52/250 [00:11<00:40,  4.86it/s, loss=1.6766, acc=20.91%]\u001b[A\n",
            "Training:  21% 52/250 [00:12<00:40,  4.86it/s, loss=1.6236, acc=21.46%]\u001b[A\n",
            "Training:  21% 53/250 [00:12<00:37,  5.30it/s, loss=1.6236, acc=21.46%]\u001b[A\n",
            "Training:  21% 53/250 [00:12<00:37,  5.30it/s, loss=1.5417, acc=21.99%]\u001b[A\n",
            "Training:  22% 54/250 [00:12<00:33,  5.82it/s, loss=1.5417, acc=21.99%]\u001b[A\n",
            "Training:  22% 54/250 [00:12<00:33,  5.82it/s, loss=1.9802, acc=22.10%]\u001b[A\n",
            "Training:  22% 55/250 [00:12<00:31,  6.27it/s, loss=1.9802, acc=22.10%]\u001b[A\n",
            "Training:  22% 55/250 [00:12<00:31,  6.27it/s, loss=1.5984, acc=22.71%]\u001b[A\n",
            "Training:  22% 56/250 [00:12<00:45,  4.23it/s, loss=1.5984, acc=22.71%]\u001b[A\n",
            "Training:  22% 56/250 [00:12<00:45,  4.23it/s, loss=1.7248, acc=22.86%]\u001b[A\n",
            "Training:  23% 57/250 [00:12<00:38,  4.97it/s, loss=1.7248, acc=22.86%]\u001b[A\n",
            "Training:  23% 57/250 [00:12<00:38,  4.97it/s, loss=1.6411, acc=23.28%]\u001b[A\n",
            "Training:  23% 58/250 [00:12<00:33,  5.66it/s, loss=1.6411, acc=23.28%]\u001b[A\n",
            "Training:  23% 58/250 [00:13<00:33,  5.66it/s, loss=1.6626, acc=23.68%]\u001b[A\n",
            "Training:  24% 59/250 [00:13<00:30,  6.26it/s, loss=1.6626, acc=23.68%]\u001b[A\n",
            "Training:  24% 59/250 [00:13<00:30,  6.26it/s, loss=1.8311, acc=23.75%]\u001b[A\n",
            "Training:  24% 60/250 [00:13<00:49,  3.87it/s, loss=1.8311, acc=23.75%]\u001b[A\n",
            "Training:  24% 60/250 [00:13<00:49,  3.87it/s, loss=1.6484, acc=24.23%]\u001b[A\n",
            "Training:  24% 61/250 [00:13<00:42,  4.48it/s, loss=1.6484, acc=24.23%]\u001b[A\n",
            "Training:  24% 61/250 [00:13<00:42,  4.48it/s, loss=1.6397, acc=24.45%]\u001b[A\n",
            "Training:  25% 62/250 [00:13<00:36,  5.19it/s, loss=1.6397, acc=24.45%]\u001b[A\n",
            "Training:  25% 62/250 [00:13<00:36,  5.19it/s, loss=1.8441, acc=24.50%]\u001b[A\n",
            "Training:  25% 63/250 [00:13<00:32,  5.78it/s, loss=1.8441, acc=24.50%]\u001b[A\n",
            "Training:  25% 63/250 [00:14<00:32,  5.78it/s, loss=1.4514, acc=24.90%]\u001b[A\n",
            "Training:  26% 64/250 [00:14<00:46,  4.00it/s, loss=1.4514, acc=24.90%]\u001b[A\n",
            "Training:  26% 64/250 [00:14<00:46,  4.00it/s, loss=1.5700, acc=25.00%]\u001b[A\n",
            "Training:  26% 65/250 [00:14<00:38,  4.75it/s, loss=1.5700, acc=25.00%]\u001b[A\n",
            "Training:  26% 65/250 [00:14<00:38,  4.75it/s, loss=1.6660, acc=25.14%]\u001b[A\n",
            "Training:  26% 66/250 [00:14<00:32,  5.61it/s, loss=1.6660, acc=25.14%]\u001b[A\n",
            "Training:  26% 66/250 [00:14<00:32,  5.61it/s, loss=1.6798, acc=25.23%]\u001b[A\n",
            "Training:  27% 67/250 [00:14<00:29,  6.26it/s, loss=1.6798, acc=25.23%]\u001b[A\n",
            "Training:  27% 67/250 [00:15<00:29,  6.26it/s, loss=1.5087, acc=25.60%]\u001b[A\n",
            "Training:  27% 68/250 [00:15<00:47,  3.80it/s, loss=1.5087, acc=25.60%]\u001b[A\n",
            "Training:  27% 68/250 [00:15<00:47,  3.80it/s, loss=1.4960, acc=26.00%]\u001b[A\n",
            "Training:  28% 69/250 [00:15<00:40,  4.48it/s, loss=1.4960, acc=26.00%]\u001b[A\n",
            "Training:  28% 69/250 [00:15<00:40,  4.48it/s, loss=1.6795, acc=26.21%]\u001b[A\n",
            "Training:  28% 70/250 [00:15<00:34,  5.28it/s, loss=1.6795, acc=26.21%]\u001b[A\n",
            "Training:  28% 70/250 [00:15<00:34,  5.28it/s, loss=1.4630, acc=26.54%]\u001b[A\n",
            "Training:  28% 70/250 [00:15<00:34,  5.28it/s, loss=1.4217, acc=26.82%]\u001b[A\n",
            "Training:  29% 72/250 [00:15<00:30,  5.83it/s, loss=1.4217, acc=26.82%]\u001b[A\n",
            "Training:  29% 72/250 [00:15<00:30,  5.83it/s, loss=1.9523, acc=26.76%]\u001b[A\n",
            "Training:  29% 72/250 [00:15<00:30,  5.83it/s, loss=1.6730, acc=26.90%]\u001b[A\n",
            "Training:  30% 74/250 [00:15<00:26,  6.76it/s, loss=1.6730, acc=26.90%]\u001b[A\n",
            "Training:  30% 74/250 [00:16<00:26,  6.76it/s, loss=1.3846, acc=27.29%]\u001b[A\n",
            "Training:  30% 74/250 [00:16<00:26,  6.76it/s, loss=1.2164, acc=27.80%]\u001b[A\n",
            "Training:  30% 76/250 [00:16<00:23,  7.36it/s, loss=1.2164, acc=27.80%]\u001b[A\n",
            "Training:  30% 76/250 [00:16<00:23,  7.36it/s, loss=1.4549, acc=28.12%]\u001b[A\n",
            "Training:  30% 76/250 [00:16<00:23,  7.36it/s, loss=1.5395, acc=28.41%]\u001b[A\n",
            "Training:  31% 78/250 [00:16<00:24,  7.06it/s, loss=1.5395, acc=28.41%]\u001b[A\n",
            "Training:  31% 78/250 [00:16<00:24,  7.06it/s, loss=1.3542, acc=28.88%]\u001b[A\n",
            "Training:  32% 79/250 [00:16<00:22,  7.47it/s, loss=1.3542, acc=28.88%]\u001b[A\n",
            "Training:  32% 79/250 [00:16<00:22,  7.47it/s, loss=1.5263, acc=29.14%]\u001b[A\n",
            "Training:  32% 80/250 [00:16<00:22,  7.40it/s, loss=1.5263, acc=29.14%]\u001b[A\n",
            "Training:  32% 80/250 [00:16<00:22,  7.40it/s, loss=1.4099, acc=29.28%]\u001b[A\n",
            "Training:  32% 81/250 [00:16<00:21,  7.84it/s, loss=1.4099, acc=29.28%]\u001b[A\n",
            "Training:  32% 81/250 [00:17<00:21,  7.84it/s, loss=1.5012, acc=29.54%]\u001b[A\n",
            "Training:  33% 82/250 [00:17<00:24,  6.96it/s, loss=1.5012, acc=29.54%]\u001b[A\n",
            "Training:  33% 82/250 [00:17<00:24,  6.96it/s, loss=1.6382, acc=29.52%]\u001b[A\n",
            "Training:  33% 83/250 [00:17<00:22,  7.54it/s, loss=1.6382, acc=29.52%]\u001b[A\n",
            "Training:  33% 83/250 [00:17<00:22,  7.54it/s, loss=1.7676, acc=29.54%]\u001b[A\n",
            "Training:  34% 84/250 [00:17<00:20,  7.91it/s, loss=1.7676, acc=29.54%]\u001b[A\n",
            "Training:  34% 84/250 [00:17<00:20,  7.91it/s, loss=1.1854, acc=29.85%]\u001b[A\n",
            "Training:  34% 85/250 [00:17<00:20,  8.14it/s, loss=1.1854, acc=29.85%]\u001b[A\n",
            "Training:  34% 85/250 [00:17<00:20,  8.14it/s, loss=1.2027, acc=30.20%]\u001b[A\n",
            "Training:  34% 86/250 [00:17<00:28,  5.81it/s, loss=1.2027, acc=30.20%]\u001b[A\n",
            "Training:  34% 86/250 [00:17<00:28,  5.81it/s, loss=1.2834, acc=30.53%]\u001b[A\n",
            "Training:  35% 87/250 [00:17<00:25,  6.47it/s, loss=1.2834, acc=30.53%]\u001b[A\n",
            "Training:  35% 87/250 [00:17<00:25,  6.47it/s, loss=1.7068, acc=30.58%]\u001b[A\n",
            "Training:  35% 88/250 [00:17<00:26,  6.04it/s, loss=1.7068, acc=30.58%]\u001b[A\n",
            "Training:  35% 88/250 [00:18<00:26,  6.04it/s, loss=1.3464, acc=30.76%]\u001b[A\n",
            "Training:  36% 89/250 [00:18<00:23,  6.77it/s, loss=1.3464, acc=30.76%]\u001b[A\n",
            "Training:  36% 89/250 [00:18<00:23,  6.77it/s, loss=1.4392, acc=31.01%]\u001b[A\n",
            "Training:  36% 90/250 [00:18<00:24,  6.52it/s, loss=1.4392, acc=31.01%]\u001b[A\n",
            "Training:  36% 90/250 [00:18<00:24,  6.52it/s, loss=1.2252, acc=31.32%]\u001b[A\n",
            "Training:  36% 90/250 [00:18<00:24,  6.52it/s, loss=1.4800, acc=31.56%]\u001b[A\n",
            "Training:  37% 92/250 [00:18<00:22,  7.12it/s, loss=1.4800, acc=31.56%]\u001b[A\n",
            "Training:  37% 92/250 [00:18<00:22,  7.12it/s, loss=1.3625, acc=31.75%]\u001b[A\n",
            "Training:  37% 93/250 [00:18<00:20,  7.60it/s, loss=1.3625, acc=31.75%]\u001b[A\n",
            "Training:  37% 93/250 [00:18<00:20,  7.60it/s, loss=1.3679, acc=31.85%]\u001b[A\n",
            "Training:  38% 94/250 [00:18<00:24,  6.38it/s, loss=1.3679, acc=31.85%]\u001b[A\n",
            "Training:  38% 94/250 [00:18<00:24,  6.38it/s, loss=1.0590, acc=32.30%]\u001b[A\n",
            "Training:  38% 95/250 [00:18<00:22,  7.02it/s, loss=1.0590, acc=32.30%]\u001b[A\n",
            "Training:  38% 95/250 [00:19<00:22,  7.02it/s, loss=1.3635, acc=32.45%]\u001b[A\n",
            "Training:  38% 96/250 [00:19<00:20,  7.47it/s, loss=1.3635, acc=32.45%]\u001b[A\n",
            "Training:  38% 96/250 [00:19<00:20,  7.47it/s, loss=1.1451, acc=32.80%]\u001b[A\n",
            "Training:  38% 96/250 [00:19<00:20,  7.47it/s, loss=1.3233, acc=32.94%]\u001b[A\n",
            "Training:  39% 98/250 [00:19<00:23,  6.47it/s, loss=1.3233, acc=32.94%]\u001b[A\n",
            "Training:  39% 98/250 [00:19<00:23,  6.47it/s, loss=1.2749, acc=33.24%]\u001b[A\n",
            "Training:  39% 98/250 [00:19<00:23,  6.47it/s, loss=1.4622, acc=33.34%]\u001b[A\n",
            "Training:  40% 100/250 [00:19<00:19,  7.60it/s, loss=1.4622, acc=33.34%]\u001b[A\n",
            "Training:  40% 100/250 [00:19<00:19,  7.60it/s, loss=1.2519, acc=33.57%]\u001b[A\n",
            "Training:  40% 101/250 [00:19<00:18,  7.93it/s, loss=1.2519, acc=33.57%]\u001b[A\n",
            "Training:  40% 101/250 [00:19<00:18,  7.93it/s, loss=1.1966, acc=33.79%]\u001b[A\n",
            "Training:  41% 102/250 [00:19<00:22,  6.62it/s, loss=1.1966, acc=33.79%]\u001b[A\n",
            "Training:  41% 102/250 [00:20<00:22,  6.62it/s, loss=1.0439, acc=34.04%]\u001b[A\n",
            "Training:  41% 102/250 [00:20<00:22,  6.62it/s, loss=1.2351, acc=34.22%]\u001b[A\n",
            "Training:  42% 104/250 [00:20<00:18,  7.82it/s, loss=1.2351, acc=34.22%]\u001b[A\n",
            "Training:  42% 104/250 [00:20<00:18,  7.82it/s, loss=1.2674, acc=34.40%]\u001b[A\n",
            "Training:  42% 104/250 [00:20<00:18,  7.82it/s, loss=1.3793, acc=34.55%]\u001b[A\n",
            "Training:  42% 106/250 [00:20<00:21,  6.69it/s, loss=1.3793, acc=34.55%]\u001b[A\n",
            "Training:  42% 106/250 [00:20<00:21,  6.69it/s, loss=1.3546, acc=34.70%]\u001b[A\n",
            "Training:  43% 107/250 [00:20<00:20,  7.15it/s, loss=1.3546, acc=34.70%]\u001b[A\n",
            "Training:  43% 107/250 [00:20<00:20,  7.15it/s, loss=1.2857, acc=34.92%]\u001b[A\n",
            "Training:  43% 108/250 [00:20<00:18,  7.62it/s, loss=1.2857, acc=34.92%]\u001b[A\n",
            "Training:  43% 108/250 [00:20<00:18,  7.62it/s, loss=1.2006, acc=35.12%]\u001b[A\n",
            "Training:  44% 109/250 [00:20<00:17,  7.99it/s, loss=1.2006, acc=35.12%]\u001b[A\n",
            "Training:  44% 109/250 [00:21<00:17,  7.99it/s, loss=1.0438, acc=35.43%]\u001b[A\n",
            "Training:  44% 110/250 [00:21<00:21,  6.54it/s, loss=1.0438, acc=35.43%]\u001b[A\n",
            "Training:  44% 110/250 [00:21<00:21,  6.54it/s, loss=1.5714, acc=35.44%]\u001b[A\n",
            "Training:  44% 110/250 [00:21<00:21,  6.54it/s, loss=1.4304, acc=35.60%]\u001b[A\n",
            "Training:  45% 112/250 [00:21<00:17,  7.68it/s, loss=1.4304, acc=35.60%]\u001b[A\n",
            "Training:  45% 112/250 [00:21<00:17,  7.68it/s, loss=1.2430, acc=35.70%]\u001b[A\n",
            "Training:  45% 113/250 [00:21<00:17,  8.05it/s, loss=1.2430, acc=35.70%]\u001b[A\n",
            "Training:  45% 113/250 [00:21<00:17,  8.05it/s, loss=1.2974, acc=35.80%]\u001b[A\n",
            "Training:  46% 114/250 [00:21<00:18,  7.50it/s, loss=1.2974, acc=35.80%]\u001b[A\n",
            "Training:  46% 114/250 [00:21<00:18,  7.50it/s, loss=1.1499, acc=35.95%]\u001b[A\n",
            "Training:  46% 115/250 [00:21<00:17,  7.94it/s, loss=1.1499, acc=35.95%]\u001b[A\n",
            "Training:  46% 115/250 [00:21<00:17,  7.94it/s, loss=1.4438, acc=35.99%]\u001b[A\n",
            "Training:  46% 116/250 [00:21<00:16,  8.27it/s, loss=1.4438, acc=35.99%]\u001b[A\n",
            "Training:  46% 116/250 [00:21<00:16,  8.27it/s, loss=1.2532, acc=36.14%]\u001b[A\n",
            "Training:  46% 116/250 [00:21<00:16,  8.27it/s, loss=1.2313, acc=36.26%]\u001b[A\n",
            "Training:  47% 118/250 [00:21<00:16,  7.78it/s, loss=1.2313, acc=36.26%]\u001b[A\n",
            "Training:  47% 118/250 [00:22<00:16,  7.78it/s, loss=1.3155, acc=36.40%]\u001b[A\n",
            "Training:  47% 118/250 [00:22<00:16,  7.78it/s, loss=1.5179, acc=36.46%]\u001b[A\n",
            "Training:  48% 120/250 [00:22<00:15,  8.56it/s, loss=1.5179, acc=36.46%]\u001b[A\n",
            "Training:  48% 120/250 [00:22<00:15,  8.56it/s, loss=1.1641, acc=36.57%]\u001b[A\n",
            "Training:  48% 120/250 [00:22<00:15,  8.56it/s, loss=1.1300, acc=36.89%]\u001b[A\n",
            "Training:  49% 122/250 [00:22<00:17,  7.34it/s, loss=1.1300, acc=36.89%]\u001b[A\n",
            "Training:  49% 122/250 [00:22<00:17,  7.34it/s, loss=1.1700, acc=37.04%]\u001b[A\n",
            "Training:  49% 122/250 [00:22<00:17,  7.34it/s, loss=1.2418, acc=37.17%]\u001b[A\n",
            "Training:  50% 124/250 [00:22<00:15,  8.04it/s, loss=1.2418, acc=37.17%]\u001b[A\n",
            "Training:  50% 124/250 [00:22<00:15,  8.04it/s, loss=1.1670, acc=37.38%]\u001b[A\n",
            "Training:  50% 125/250 [00:22<00:15,  8.32it/s, loss=1.1670, acc=37.38%]\u001b[A\n",
            "Training:  50% 125/250 [00:23<00:15,  8.32it/s, loss=1.0844, acc=37.57%]\u001b[A\n",
            "Training:  50% 126/250 [00:23<00:19,  6.29it/s, loss=1.0844, acc=37.57%]\u001b[A\n",
            "Training:  50% 126/250 [00:23<00:19,  6.29it/s, loss=1.1183, acc=37.77%]\u001b[A\n",
            "Training:  50% 126/250 [00:23<00:19,  6.29it/s, loss=1.0208, acc=37.94%]\u001b[A\n",
            "Training:  51% 128/250 [00:23<00:16,  7.38it/s, loss=1.0208, acc=37.94%]\u001b[A\n",
            "Training:  51% 128/250 [00:23<00:16,  7.38it/s, loss=1.1011, acc=38.08%]\u001b[A\n",
            "Training:  51% 128/250 [00:23<00:16,  7.38it/s, loss=0.9641, acc=38.27%]\u001b[A\n",
            "Training:  52% 130/250 [00:23<00:16,  7.40it/s, loss=0.9641, acc=38.27%]\u001b[A\n",
            "Training:  52% 130/250 [00:23<00:16,  7.40it/s, loss=1.2751, acc=38.38%]\u001b[A\n",
            "Training:  52% 131/250 [00:23<00:15,  7.77it/s, loss=1.2751, acc=38.38%]\u001b[A\n",
            "Training:  52% 131/250 [00:23<00:15,  7.77it/s, loss=0.9319, acc=38.64%]\u001b[A\n",
            "Training:  52% 131/250 [00:23<00:15,  7.77it/s, loss=1.1207, acc=38.77%]\u001b[A\n",
            "Training:  53% 133/250 [00:23<00:13,  8.48it/s, loss=1.1207, acc=38.77%]\u001b[A\n",
            "Training:  53% 133/250 [00:24<00:13,  8.48it/s, loss=1.3725, acc=38.88%]\u001b[A\n",
            "Training:  54% 134/250 [00:24<00:15,  7.56it/s, loss=1.3725, acc=38.88%]\u001b[A\n",
            "Training:  54% 134/250 [00:24<00:15,  7.56it/s, loss=1.0969, acc=39.07%]\u001b[A\n",
            "Training:  54% 135/250 [00:24<00:14,  7.89it/s, loss=1.0969, acc=39.07%]\u001b[A\n",
            "Training:  54% 135/250 [00:24<00:14,  7.89it/s, loss=1.0293, acc=39.32%]\u001b[A\n",
            "Training:  54% 136/250 [00:24<00:13,  8.27it/s, loss=1.0293, acc=39.32%]\u001b[A\n",
            "Training:  54% 136/250 [00:24<00:13,  8.27it/s, loss=1.1515, acc=39.39%]\u001b[A\n",
            "Training:  55% 137/250 [00:24<00:13,  8.50it/s, loss=1.1515, acc=39.39%]\u001b[A\n",
            "Training:  55% 137/250 [00:24<00:13,  8.50it/s, loss=0.8753, acc=39.56%]\u001b[A\n",
            "Training:  55% 138/250 [00:24<00:18,  6.13it/s, loss=0.8753, acc=39.56%]\u001b[A\n",
            "Training:  55% 138/250 [00:24<00:18,  6.13it/s, loss=1.0180, acc=39.68%]\u001b[A\n",
            "Training:  56% 139/250 [00:24<00:16,  6.81it/s, loss=1.0180, acc=39.68%]\u001b[A\n",
            "Training:  56% 139/250 [00:24<00:16,  6.81it/s, loss=0.8508, acc=39.89%]\u001b[A\n",
            "Training:  56% 140/250 [00:24<00:14,  7.40it/s, loss=0.8508, acc=39.89%]\u001b[A\n",
            "Training:  56% 140/250 [00:24<00:14,  7.40it/s, loss=1.1765, acc=40.07%]\u001b[A\n",
            "Training:  56% 140/250 [00:25<00:14,  7.40it/s, loss=1.2055, acc=40.21%]\u001b[A\n",
            "Training:  57% 142/250 [00:25<00:17,  6.11it/s, loss=1.2055, acc=40.21%]\u001b[A\n",
            "Training:  57% 142/250 [00:25<00:17,  6.11it/s, loss=1.1259, acc=40.38%]\u001b[A\n",
            "Training:  57% 142/250 [00:25<00:17,  6.11it/s, loss=1.5486, acc=40.32%]\u001b[A\n",
            "Training:  58% 144/250 [00:25<00:14,  7.34it/s, loss=1.5486, acc=40.32%]\u001b[A\n",
            "Training:  58% 144/250 [00:25<00:14,  7.34it/s, loss=1.2456, acc=40.43%]\u001b[A\n",
            "Training:  58% 144/250 [00:25<00:14,  7.34it/s, loss=1.2489, acc=40.50%]\u001b[A\n",
            "Training:  58% 146/250 [00:25<00:18,  5.61it/s, loss=1.2489, acc=40.50%]\u001b[A\n",
            "Training:  58% 146/250 [00:26<00:18,  5.61it/s, loss=0.9809, acc=40.62%]\u001b[A\n",
            "Training:  59% 147/250 [00:26<00:17,  5.95it/s, loss=0.9809, acc=40.62%]\u001b[A\n",
            "Training:  59% 147/250 [00:26<00:17,  5.95it/s, loss=1.2162, acc=40.73%]\u001b[A\n",
            "Training:  59% 148/250 [00:26<00:16,  6.35it/s, loss=1.2162, acc=40.73%]\u001b[A\n",
            "Training:  59% 148/250 [00:26<00:16,  6.35it/s, loss=1.1053, acc=40.90%]\u001b[A\n",
            "Training:  60% 149/250 [00:26<00:15,  6.69it/s, loss=1.1053, acc=40.90%]\u001b[A\n",
            "Training:  60% 149/250 [00:26<00:15,  6.69it/s, loss=1.1466, acc=40.94%]\u001b[A\n",
            "Training:  60% 150/250 [00:26<00:23,  4.34it/s, loss=1.1466, acc=40.94%]\u001b[A\n",
            "Training:  60% 150/250 [00:26<00:23,  4.34it/s, loss=1.1167, acc=41.02%]\u001b[A\n",
            "Training:  60% 151/250 [00:26<00:20,  4.95it/s, loss=1.1167, acc=41.02%]\u001b[A\n",
            "Training:  60% 151/250 [00:27<00:20,  4.95it/s, loss=1.0791, acc=41.16%]\u001b[A\n",
            "Training:  61% 152/250 [00:27<00:17,  5.51it/s, loss=1.0791, acc=41.16%]\u001b[A\n",
            "Training:  61% 152/250 [00:27<00:17,  5.51it/s, loss=0.9698, acc=41.30%]\u001b[A\n",
            "Training:  61% 153/250 [00:27<00:15,  6.09it/s, loss=0.9698, acc=41.30%]\u001b[A\n",
            "Training:  61% 153/250 [00:27<00:15,  6.09it/s, loss=0.9483, acc=41.44%]\u001b[A\n",
            "Training:  62% 154/250 [00:27<00:23,  4.13it/s, loss=0.9483, acc=41.44%]\u001b[A\n",
            "Training:  62% 154/250 [00:27<00:23,  4.13it/s, loss=1.2978, acc=41.55%]\u001b[A\n",
            "Training:  62% 155/250 [00:27<00:19,  4.81it/s, loss=1.2978, acc=41.55%]\u001b[A\n",
            "Training:  62% 155/250 [00:27<00:19,  4.81it/s, loss=1.0250, acc=41.65%]\u001b[A\n",
            "Training:  62% 156/250 [00:27<00:16,  5.54it/s, loss=1.0250, acc=41.65%]\u001b[A\n",
            "Training:  62% 156/250 [00:27<00:16,  5.54it/s, loss=0.9789, acc=41.72%]\u001b[A\n",
            "Training:  63% 157/250 [00:27<00:14,  6.21it/s, loss=0.9789, acc=41.72%]\u001b[A\n",
            "Training:  63% 157/250 [00:28<00:14,  6.21it/s, loss=1.2382, acc=41.79%]\u001b[A\n",
            "Training:  63% 158/250 [00:28<00:24,  3.82it/s, loss=1.2382, acc=41.79%]\u001b[A\n",
            "Training:  63% 158/250 [00:28<00:24,  3.82it/s, loss=1.0988, acc=41.82%]\u001b[A\n",
            "Training:  64% 159/250 [00:28<00:20,  4.51it/s, loss=1.0988, acc=41.82%]\u001b[A\n",
            "Training:  64% 159/250 [00:28<00:20,  4.51it/s, loss=1.3479, acc=41.82%]\u001b[A\n",
            "Training:  64% 160/250 [00:28<00:18,  4.99it/s, loss=1.3479, acc=41.82%]\u001b[A\n",
            "Training:  64% 160/250 [00:28<00:18,  4.99it/s, loss=1.1302, acc=41.94%]\u001b[A\n",
            "Training:  64% 161/250 [00:28<00:15,  5.60it/s, loss=1.1302, acc=41.94%]\u001b[A\n",
            "Training:  64% 161/250 [00:29<00:15,  5.60it/s, loss=1.3768, acc=41.96%]\u001b[A\n",
            "Training:  65% 162/250 [00:29<00:22,  3.88it/s, loss=1.3768, acc=41.96%]\u001b[A\n",
            "Training:  65% 162/250 [00:29<00:22,  3.88it/s, loss=1.0237, acc=42.01%]\u001b[A\n",
            "Training:  65% 163/250 [00:29<00:18,  4.68it/s, loss=1.0237, acc=42.01%]\u001b[A\n",
            "Training:  65% 163/250 [00:29<00:18,  4.68it/s, loss=0.9670, acc=42.13%]\u001b[A\n",
            "Training:  66% 164/250 [00:29<00:16,  5.35it/s, loss=0.9670, acc=42.13%]\u001b[A\n",
            "Training:  66% 164/250 [00:29<00:16,  5.35it/s, loss=0.7766, acc=42.27%]\u001b[A\n",
            "Training:  66% 165/250 [00:29<00:14,  5.80it/s, loss=0.7766, acc=42.27%]\u001b[A\n",
            "Training:  66% 165/250 [00:30<00:14,  5.80it/s, loss=0.9622, acc=42.34%]\u001b[A\n",
            "Training:  66% 166/250 [00:30<00:26,  3.22it/s, loss=0.9622, acc=42.34%]\u001b[A\n",
            "Training:  66% 166/250 [00:30<00:26,  3.22it/s, loss=1.2969, acc=42.35%]\u001b[A\n",
            "Training:  67% 167/250 [00:30<00:21,  3.89it/s, loss=1.2969, acc=42.35%]\u001b[A\n",
            "Training:  67% 167/250 [00:30<00:21,  3.89it/s, loss=1.1710, acc=42.43%]\u001b[A\n",
            "Training:  67% 168/250 [00:30<00:17,  4.71it/s, loss=1.1710, acc=42.43%]\u001b[A\n",
            "Training:  67% 168/250 [00:30<00:17,  4.71it/s, loss=0.9963, acc=42.51%]\u001b[A\n",
            "Training:  68% 169/250 [00:30<00:15,  5.36it/s, loss=0.9963, acc=42.51%]\u001b[A\n",
            "Training:  68% 169/250 [00:31<00:15,  5.36it/s, loss=0.8577, acc=42.65%]\u001b[A\n",
            "Training:  68% 170/250 [00:31<00:21,  3.70it/s, loss=0.8577, acc=42.65%]\u001b[A\n",
            "Training:  68% 170/250 [00:31<00:21,  3.70it/s, loss=0.8922, acc=42.78%]\u001b[A\n",
            "Training:  68% 170/250 [00:31<00:21,  3.70it/s, loss=0.9004, acc=42.90%]\u001b[A\n",
            "Training:  69% 172/250 [00:31<00:14,  5.24it/s, loss=0.9004, acc=42.90%]\u001b[A\n",
            "Training:  69% 172/250 [00:31<00:14,  5.24it/s, loss=0.7944, acc=42.99%]\u001b[A\n",
            "Training:  69% 173/250 [00:31<00:12,  5.93it/s, loss=0.7944, acc=42.99%]\u001b[A\n",
            "Training:  69% 173/250 [00:31<00:12,  5.93it/s, loss=0.9817, acc=43.09%]\u001b[A\n",
            "Training:  70% 174/250 [00:31<00:14,  5.33it/s, loss=0.9817, acc=43.09%]\u001b[A\n",
            "Training:  70% 174/250 [00:31<00:14,  5.33it/s, loss=1.0996, acc=43.18%]\u001b[A\n",
            "Training:  70% 175/250 [00:31<00:12,  6.00it/s, loss=1.0996, acc=43.18%]\u001b[A\n",
            "Training:  70% 175/250 [00:31<00:12,  6.00it/s, loss=1.1250, acc=43.24%]\u001b[A\n",
            "Training:  70% 175/250 [00:32<00:12,  6.00it/s, loss=0.9278, acc=43.34%]\u001b[A\n",
            "Training:  71% 177/250 [00:32<00:09,  7.46it/s, loss=0.9278, acc=43.34%]\u001b[A\n",
            "Training:  71% 177/250 [00:32<00:09,  7.46it/s, loss=0.7556, acc=43.52%]\u001b[A\n",
            "Training:  71% 178/250 [00:32<00:09,  7.80it/s, loss=0.7556, acc=43.52%]\u001b[A\n",
            "Training:  71% 178/250 [00:32<00:09,  7.80it/s, loss=0.7149, acc=43.63%]\u001b[A\n",
            "Training:  72% 179/250 [00:32<00:08,  8.22it/s, loss=0.7149, acc=43.63%]\u001b[A\n",
            "Training:  72% 179/250 [00:32<00:08,  8.22it/s, loss=0.7941, acc=43.78%]\u001b[A\n",
            "Training:  72% 179/250 [00:32<00:08,  8.22it/s, loss=1.3867, acc=43.77%]\u001b[A\n",
            "Training:  72% 181/250 [00:32<00:07,  8.81it/s, loss=1.3867, acc=43.77%]\u001b[A\n",
            "Training:  72% 181/250 [00:32<00:07,  8.81it/s, loss=1.1213, acc=43.82%]\u001b[A\n",
            "Training:  73% 182/250 [00:32<00:12,  5.54it/s, loss=1.1213, acc=43.82%]\u001b[A\n",
            "Training:  73% 182/250 [00:32<00:12,  5.54it/s, loss=0.7450, acc=43.99%]\u001b[A\n",
            "Training:  73% 182/250 [00:33<00:12,  5.54it/s, loss=0.9915, acc=44.02%]\u001b[A\n",
            "Training:  74% 184/250 [00:33<00:09,  6.84it/s, loss=0.9915, acc=44.02%]\u001b[A\n",
            "Training:  74% 184/250 [00:33<00:09,  6.84it/s, loss=0.7826, acc=44.14%]\u001b[A\n",
            "Training:  74% 185/250 [00:33<00:08,  7.33it/s, loss=0.7826, acc=44.14%]\u001b[A\n",
            "Training:  74% 185/250 [00:33<00:08,  7.33it/s, loss=1.1062, acc=44.24%]\u001b[A\n",
            "Training:  74% 186/250 [00:33<00:10,  5.90it/s, loss=1.1062, acc=44.24%]\u001b[A\n",
            "Training:  74% 186/250 [00:33<00:10,  5.90it/s, loss=0.9357, acc=44.35%]\u001b[A\n",
            "Training:  75% 187/250 [00:33<00:09,  6.51it/s, loss=0.9357, acc=44.35%]\u001b[A\n",
            "Training:  75% 187/250 [00:33<00:09,  6.51it/s, loss=0.8644, acc=44.48%]\u001b[A\n",
            "Training:  75% 188/250 [00:33<00:08,  7.16it/s, loss=0.8644, acc=44.48%]\u001b[A\n",
            "Training:  75% 188/250 [00:33<00:08,  7.16it/s, loss=0.9745, acc=44.61%]\u001b[A\n",
            "Training:  76% 189/250 [00:33<00:07,  7.66it/s, loss=0.9745, acc=44.61%]\u001b[A\n",
            "Training:  76% 189/250 [00:33<00:07,  7.66it/s, loss=0.7072, acc=44.77%]\u001b[A\n",
            "Training:  76% 190/250 [00:33<00:10,  6.00it/s, loss=0.7072, acc=44.77%]\u001b[A\n",
            "Training:  76% 190/250 [00:34<00:10,  6.00it/s, loss=0.9637, acc=44.85%]\u001b[A\n",
            "Training:  76% 190/250 [00:34<00:10,  6.00it/s, loss=0.9515, acc=44.95%]\u001b[A\n",
            "Training:  77% 192/250 [00:34<00:07,  7.25it/s, loss=0.9515, acc=44.95%]\u001b[A\n",
            "Training:  77% 192/250 [00:34<00:07,  7.25it/s, loss=1.1628, acc=44.98%]\u001b[A\n",
            "Training:  77% 193/250 [00:34<00:07,  7.59it/s, loss=1.1628, acc=44.98%]\u001b[A\n",
            "Training:  77% 193/250 [00:34<00:07,  7.59it/s, loss=0.7751, acc=45.15%]\u001b[A\n",
            "Training:  78% 194/250 [00:34<00:09,  6.10it/s, loss=0.7751, acc=45.15%]\u001b[A\n",
            "Training:  78% 194/250 [00:34<00:09,  6.10it/s, loss=0.7284, acc=45.34%]\u001b[A\n",
            "Training:  78% 194/250 [00:34<00:09,  6.10it/s, loss=0.7753, acc=45.44%]\u001b[A\n",
            "Training:  78% 196/250 [00:34<00:07,  7.19it/s, loss=0.7753, acc=45.44%]\u001b[A\n",
            "Training:  78% 196/250 [00:34<00:07,  7.19it/s, loss=1.1545, acc=45.48%]\u001b[A\n",
            "Training:  79% 197/250 [00:34<00:07,  7.49it/s, loss=1.1545, acc=45.48%]\u001b[A\n",
            "Training:  79% 197/250 [00:35<00:07,  7.49it/s, loss=0.9491, acc=45.55%]\u001b[A\n",
            "Training:  79% 198/250 [00:35<00:07,  7.26it/s, loss=0.9491, acc=45.55%]\u001b[A\n",
            "Training:  79% 198/250 [00:35<00:07,  7.26it/s, loss=1.1651, acc=45.62%]\u001b[A\n",
            "Training:  79% 198/250 [00:35<00:07,  7.26it/s, loss=0.9804, acc=45.70%]\u001b[A\n",
            "Training:  80% 200/250 [00:35<00:06,  8.14it/s, loss=0.9804, acc=45.70%]\u001b[A\n",
            "Training:  80% 200/250 [00:35<00:06,  8.14it/s, loss=1.1845, acc=45.77%]\u001b[A\n",
            "Training:  80% 201/250 [00:35<00:05,  8.37it/s, loss=1.1845, acc=45.77%]\u001b[A\n",
            "Training:  80% 201/250 [00:35<00:05,  8.37it/s, loss=1.1708, acc=45.81%]\u001b[A\n",
            "Training:  81% 202/250 [00:35<00:05,  8.55it/s, loss=1.1708, acc=45.81%]\u001b[A\n",
            "Training:  81% 202/250 [00:35<00:05,  8.55it/s, loss=0.6823, acc=45.92%]\u001b[A\n",
            "Training:  81% 202/250 [00:35<00:05,  8.55it/s, loss=1.0002, acc=45.99%]\u001b[A\n",
            "Training:  82% 204/250 [00:35<00:05,  9.07it/s, loss=1.0002, acc=45.99%]\u001b[A\n",
            "Training:  82% 204/250 [00:35<00:05,  9.07it/s, loss=1.0888, acc=46.02%]\u001b[A\n",
            "Training:  82% 205/250 [00:35<00:05,  8.59it/s, loss=1.0888, acc=46.02%]\u001b[A\n",
            "Training:  82% 205/250 [00:36<00:05,  8.59it/s, loss=1.0480, acc=46.10%]\u001b[A\n",
            "Training:  82% 206/250 [00:36<00:07,  5.57it/s, loss=1.0480, acc=46.10%]\u001b[A\n",
            "Training:  82% 206/250 [00:36<00:07,  5.57it/s, loss=1.2452, acc=46.14%]\u001b[A\n",
            "Training:  83% 207/250 [00:36<00:06,  6.28it/s, loss=1.2452, acc=46.14%]\u001b[A\n",
            "Training:  83% 207/250 [00:36<00:06,  6.28it/s, loss=0.8267, acc=46.24%]\u001b[A\n",
            "Training:  83% 208/250 [00:36<00:06,  6.94it/s, loss=0.8267, acc=46.24%]\u001b[A\n",
            "Training:  83% 208/250 [00:36<00:06,  6.94it/s, loss=1.1005, acc=46.28%]\u001b[A\n",
            "Training:  84% 209/250 [00:36<00:05,  7.34it/s, loss=1.1005, acc=46.28%]\u001b[A\n",
            "Training:  84% 209/250 [00:36<00:05,  7.34it/s, loss=0.7607, acc=46.37%]\u001b[A\n",
            "Training:  84% 210/250 [00:36<00:06,  6.47it/s, loss=0.7607, acc=46.37%]\u001b[A\n",
            "Training:  84% 210/250 [00:36<00:06,  6.47it/s, loss=1.0645, acc=46.37%]\u001b[A\n",
            "Training:  84% 211/250 [00:36<00:05,  7.09it/s, loss=1.0645, acc=46.37%]\u001b[A\n",
            "Training:  84% 211/250 [00:36<00:05,  7.09it/s, loss=0.9852, acc=46.45%]\u001b[A\n",
            "Training:  84% 211/250 [00:36<00:05,  7.09it/s, loss=0.8772, acc=46.54%]\u001b[A\n",
            "Training:  85% 213/250 [00:36<00:04,  8.31it/s, loss=0.8772, acc=46.54%]\u001b[A\n",
            "Training:  85% 213/250 [00:37<00:04,  8.31it/s, loss=0.9544, acc=46.64%]\u001b[A\n",
            "Training:  86% 214/250 [00:37<00:05,  6.74it/s, loss=0.9544, acc=46.64%]\u001b[A\n",
            "Training:  86% 214/250 [00:37<00:05,  6.74it/s, loss=0.8320, acc=46.74%]\u001b[A\n",
            "Training:  86% 215/250 [00:37<00:04,  7.31it/s, loss=0.8320, acc=46.74%]\u001b[A\n",
            "Training:  86% 215/250 [00:37<00:04,  7.31it/s, loss=1.0463, acc=46.79%]\u001b[A\n",
            "Training:  86% 216/250 [00:37<00:04,  7.75it/s, loss=1.0463, acc=46.79%]\u001b[A\n",
            "Training:  86% 216/250 [00:37<00:04,  7.75it/s, loss=1.0124, acc=46.90%]\u001b[A\n",
            "Training:  87% 217/250 [00:37<00:04,  8.06it/s, loss=1.0124, acc=46.90%]\u001b[A\n",
            "Training:  87% 217/250 [00:37<00:04,  8.06it/s, loss=1.1909, acc=46.89%]\u001b[A\n",
            "Training:  87% 218/250 [00:37<00:04,  7.37it/s, loss=1.1909, acc=46.89%]\u001b[A\n",
            "Training:  87% 218/250 [00:37<00:04,  7.37it/s, loss=0.9376, acc=46.96%]\u001b[A\n",
            "Training:  88% 219/250 [00:37<00:03,  7.85it/s, loss=0.9376, acc=46.96%]\u001b[A\n",
            "Training:  88% 219/250 [00:37<00:03,  7.85it/s, loss=1.0047, acc=47.02%]\u001b[A\n",
            "Training:  88% 219/250 [00:37<00:03,  7.85it/s, loss=1.0153, acc=47.12%]\u001b[A\n",
            "Training:  88% 221/250 [00:37<00:03,  8.73it/s, loss=1.0153, acc=47.12%]\u001b[A\n",
            "Training:  88% 221/250 [00:38<00:03,  8.73it/s, loss=0.8045, acc=47.23%]\u001b[A\n",
            "Training:  89% 222/250 [00:38<00:04,  6.64it/s, loss=0.8045, acc=47.23%]\u001b[A\n",
            "Training:  89% 222/250 [00:38<00:04,  6.64it/s, loss=0.9288, acc=47.31%]\u001b[A\n",
            "Training:  89% 223/250 [00:38<00:03,  7.10it/s, loss=0.9288, acc=47.31%]\u001b[A\n",
            "Training:  89% 223/250 [00:38<00:03,  7.10it/s, loss=0.8974, acc=47.41%]\u001b[A\n",
            "Training:  90% 224/250 [00:38<00:03,  7.58it/s, loss=0.8974, acc=47.41%]\u001b[A\n",
            "Training:  90% 224/250 [00:38<00:03,  7.58it/s, loss=0.7172, acc=47.51%]\u001b[A\n",
            "Training:  90% 225/250 [00:38<00:03,  8.05it/s, loss=0.7172, acc=47.51%]\u001b[A\n",
            "Training:  90% 225/250 [00:38<00:03,  8.05it/s, loss=1.0509, acc=47.55%]\u001b[A\n",
            "Training:  90% 226/250 [00:38<00:03,  7.38it/s, loss=1.0509, acc=47.55%]\u001b[A\n",
            "Training:  90% 226/250 [00:38<00:03,  7.38it/s, loss=0.8235, acc=47.63%]\u001b[A\n",
            "Training:  90% 226/250 [00:38<00:03,  7.38it/s, loss=1.0521, acc=47.67%]\u001b[A\n",
            "Training:  91% 228/250 [00:38<00:02,  8.31it/s, loss=1.0521, acc=47.67%]\u001b[A\n",
            "Training:  91% 228/250 [00:39<00:02,  8.31it/s, loss=0.8772, acc=47.79%]\u001b[A\n",
            "Training:  92% 229/250 [00:39<00:02,  8.51it/s, loss=0.8772, acc=47.79%]\u001b[A\n",
            "Training:  92% 229/250 [00:39<00:02,  8.51it/s, loss=1.2592, acc=47.76%]\u001b[A\n",
            "Training:  92% 230/250 [00:39<00:02,  6.99it/s, loss=1.2592, acc=47.76%]\u001b[A\n",
            "Training:  92% 230/250 [00:39<00:02,  6.99it/s, loss=1.0406, acc=47.84%]\u001b[A\n",
            "Training:  92% 231/250 [00:39<00:02,  7.50it/s, loss=1.0406, acc=47.84%]\u001b[A\n",
            "Training:  92% 231/250 [00:39<00:02,  7.50it/s, loss=1.0340, acc=47.91%]\u001b[A\n",
            "Training:  93% 232/250 [00:39<00:02,  8.02it/s, loss=1.0340, acc=47.91%]\u001b[A\n",
            "Training:  93% 232/250 [00:39<00:02,  8.02it/s, loss=0.9115, acc=47.97%]\u001b[A\n",
            "Training:  93% 233/250 [00:39<00:02,  8.28it/s, loss=0.9115, acc=47.97%]\u001b[A\n",
            "Training:  93% 233/250 [00:39<00:02,  8.28it/s, loss=0.9839, acc=48.01%]\u001b[A\n",
            "Training:  94% 234/250 [00:39<00:01,  8.25it/s, loss=0.9839, acc=48.01%]\u001b[A\n",
            "Training:  94% 234/250 [00:39<00:01,  8.25it/s, loss=0.8317, acc=48.10%]\u001b[A\n",
            "Training:  94% 235/250 [00:39<00:01,  7.74it/s, loss=0.8317, acc=48.10%]\u001b[A\n",
            "Training:  94% 235/250 [00:39<00:01,  7.74it/s, loss=0.8086, acc=48.16%]\u001b[A\n",
            "Training:  94% 235/250 [00:40<00:01,  7.74it/s, loss=0.8748, acc=48.19%]\u001b[A\n",
            "Training:  95% 237/250 [00:40<00:01,  8.55it/s, loss=0.8748, acc=48.19%]\u001b[A\n",
            "Training:  95% 237/250 [00:40<00:01,  8.55it/s, loss=1.0312, acc=48.28%]\u001b[A\n",
            "Training:  95% 238/250 [00:40<00:01,  8.72it/s, loss=1.0312, acc=48.28%]\u001b[A\n",
            "Training:  95% 238/250 [00:40<00:01,  8.72it/s, loss=0.6832, acc=48.38%]\u001b[A\n",
            "Training:  96% 239/250 [00:40<00:01,  8.09it/s, loss=0.6832, acc=48.38%]\u001b[A\n",
            "Training:  96% 239/250 [00:40<00:01,  8.09it/s, loss=0.8814, acc=48.44%]\u001b[A\n",
            "Training:  96% 240/250 [00:40<00:01,  8.17it/s, loss=0.8814, acc=48.44%]\u001b[A\n",
            "Training:  96% 240/250 [00:40<00:01,  8.17it/s, loss=1.1185, acc=48.46%]\u001b[A\n",
            "Training:  96% 241/250 [00:40<00:01,  5.81it/s, loss=1.1185, acc=48.46%]\u001b[A\n",
            "Training:  96% 241/250 [00:40<00:01,  5.81it/s, loss=0.7248, acc=48.59%]\u001b[A\n",
            "Training:  97% 242/250 [00:40<00:01,  5.99it/s, loss=0.7248, acc=48.59%]\u001b[A\n",
            "Training:  97% 242/250 [00:41<00:01,  5.99it/s, loss=0.9159, acc=48.64%]\u001b[A\n",
            "Training:  97% 243/250 [00:41<00:01,  6.52it/s, loss=0.9159, acc=48.64%]\u001b[A\n",
            "Training:  97% 243/250 [00:41<00:01,  6.52it/s, loss=0.6565, acc=48.76%]\u001b[A\n",
            "Training:  98% 244/250 [00:41<00:00,  6.74it/s, loss=0.6565, acc=48.76%]\u001b[A\n",
            "Training:  98% 244/250 [00:41<00:00,  6.74it/s, loss=0.7808, acc=48.84%]\u001b[A\n",
            "Training:  98% 245/250 [00:41<00:01,  4.18it/s, loss=0.7808, acc=48.84%]\u001b[A\n",
            "Training:  98% 245/250 [00:41<00:01,  4.18it/s, loss=0.6810, acc=48.95%]\u001b[A\n",
            "Training:  98% 246/250 [00:41<00:00,  5.01it/s, loss=0.6810, acc=48.95%]\u001b[A\n",
            "Training:  98% 246/250 [00:41<00:00,  5.01it/s, loss=0.9798, acc=49.01%]\u001b[A\n",
            "Training:  99% 247/250 [00:41<00:00,  5.86it/s, loss=0.9798, acc=49.01%]\u001b[A\n",
            "Training:  99% 247/250 [00:41<00:00,  5.86it/s, loss=0.9973, acc=49.03%]\u001b[A\n",
            "Training:  99% 247/250 [00:42<00:00,  5.86it/s, loss=0.6550, acc=49.12%]\u001b[A\n",
            "Training: 100% 249/250 [00:42<00:00,  6.81it/s, loss=0.6550, acc=49.12%]\u001b[A\n",
            "Training: 100% 250/250 [00:42<00:00,  5.92it/s, loss=1.0183, acc=49.17%]\n",
            "Validating: 100% 32/32 [00:05<00:00,  5.70it/s]\n",
            "Epoch 1: Train Loss=1.3696, Acc=49.17% | Val Loss=0.7977, Acc=67.80%\n",
            "--> New best model saved with Val Acc: 67.80%\n",
            "\n",
            "Epoch 2/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.48it/s, loss=0.9259, acc=68.17%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.53it/s]\n",
            "Epoch 2: Train Loss=0.7556, Acc=68.17% | Val Loss=0.7080, Acc=69.80%\n",
            "--> New best model saved with Val Acc: 69.80%\n",
            "\n",
            "Epoch 3/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.51it/s, loss=0.5838, acc=73.92%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.98it/s]\n",
            "Epoch 3: Train Loss=0.6211, Acc=73.92% | Val Loss=0.6027, Acc=73.90%\n",
            "--> New best model saved with Val Acc: 73.90%\n",
            "\n",
            "Epoch 4/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.52it/s, loss=0.5624, acc=76.71%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.93it/s]\n",
            "Epoch 4: Train Loss=0.5520, Acc=76.71% | Val Loss=0.5623, Acc=75.00%\n",
            "--> New best model saved with Val Acc: 75.00%\n",
            "\n",
            "Epoch 5/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.54it/s, loss=0.5362, acc=78.27%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  7.07it/s]\n",
            "Epoch 5: Train Loss=0.5054, Acc=78.27% | Val Loss=0.5842, Acc=74.60%\n",
            "\n",
            "Epoch 6/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.43it/s, loss=0.3503, acc=80.75%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.99it/s]\n",
            "Epoch 6: Train Loss=0.4668, Acc=80.75% | Val Loss=0.4891, Acc=80.00%\n",
            "--> New best model saved with Val Acc: 80.00%\n",
            "\n",
            "Epoch 7/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.53it/s, loss=0.6384, acc=82.49%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.95it/s]\n",
            "Epoch 7: Train Loss=0.4212, Acc=82.49% | Val Loss=0.5277, Acc=79.70%\n",
            "\n",
            "Epoch 8/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.52it/s, loss=0.4410, acc=84.02%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.73it/s]\n",
            "Epoch 8: Train Loss=0.3802, Acc=84.02% | Val Loss=0.5700, Acc=76.30%\n",
            "\n",
            "Epoch 9/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.52it/s, loss=0.3441, acc=84.44%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.93it/s]\n",
            "Epoch 9: Train Loss=0.3675, Acc=84.44% | Val Loss=0.4824, Acc=80.80%\n",
            "--> New best model saved with Val Acc: 80.80%\n",
            "\n",
            "Epoch 10/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.51it/s, loss=0.4184, acc=85.12%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  7.01it/s]\n",
            "Epoch 10: Train Loss=0.3467, Acc=85.12% | Val Loss=0.4953, Acc=80.50%\n",
            "\n",
            "Epoch 11/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.48it/s, loss=0.2669, acc=87.27%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.96it/s]\n",
            "Epoch 11: Train Loss=0.3097, Acc=87.27% | Val Loss=0.6031, Acc=77.90%\n",
            "\n",
            "Epoch 12/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.44it/s, loss=0.2174, acc=87.11%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  7.01it/s]\n",
            "Epoch 12: Train Loss=0.3190, Acc=87.11% | Val Loss=0.5204, Acc=80.20%\n",
            "\n",
            "Epoch 13/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.48it/s, loss=0.3304, acc=88.84%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.76it/s]\n",
            "Epoch 13: Train Loss=0.2771, Acc=88.84% | Val Loss=0.5185, Acc=79.10%\n",
            "\n",
            "Epoch 14/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.52it/s, loss=0.1363, acc=90.17%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.95it/s]\n",
            "Epoch 14: Train Loss=0.2454, Acc=90.17% | Val Loss=0.5033, Acc=81.70%\n",
            "--> New best model saved with Val Acc: 81.70%\n",
            "\n",
            "Epoch 15/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.52it/s, loss=0.2087, acc=90.80%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.81it/s]\n",
            "Epoch 15: Train Loss=0.2325, Acc=90.80% | Val Loss=0.4815, Acc=82.30%\n",
            "--> New best model saved with Val Acc: 82.30%\n",
            "\n",
            "Epoch 16/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.47it/s, loss=0.1776, acc=91.51%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  7.02it/s]\n",
            "Epoch 16: Train Loss=0.2081, Acc=91.51% | Val Loss=0.5326, Acc=81.60%\n",
            "\n",
            "Epoch 17/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.50it/s, loss=0.3318, acc=91.52%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.99it/s]\n",
            "Epoch 17: Train Loss=0.2063, Acc=91.52% | Val Loss=0.4857, Acc=81.50%\n",
            "\n",
            "Epoch 18/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.53it/s, loss=0.3230, acc=93.07%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.95it/s]\n",
            "Epoch 18: Train Loss=0.1768, Acc=93.07% | Val Loss=0.5676, Acc=81.50%\n",
            "\n",
            "Epoch 19/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.51it/s, loss=0.1071, acc=93.61%]\n",
            "Validating: 100% 32/32 [00:04<00:00,  6.70it/s]\n",
            "Epoch 19: Train Loss=0.1680, Acc=93.61% | Val Loss=0.6519, Acc=78.80%\n",
            "\n",
            "Epoch 20/20\n",
            "Training: 100% 250/250 [00:38<00:00,  6.56it/s, loss=0.0542, acc=93.65%]\n",
            "Validating: 100% 32/32 [00:05<00:00,  6.37it/s]\n",
            "Epoch 20: Train Loss=0.1711, Acc=93.65% | Val Loss=0.5597, Acc=81.80%\n",
            "\n",
            "--- Training Finished. Best Val Acc: 82.30% ---\n",
            "\n",
            "--- Starting Testing with the Best Model ---\n",
            "Model loaded from outputs/clip/best_model.pth\n",
            "Testing: 100% 32/32 [00:05<00:00,  5.65it/s]\n",
            "\n",
            "Test Results: Loss=0.4700, Accuracy=83.92%\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "              ADM       0.76      0.67      0.71       101\n",
            "             DDPM       0.93      0.95      0.94       100\n",
            "Diff-ProjectedGAN       0.65      0.67      0.66       100\n",
            "   Diff-StyleGAN2       0.98      0.98      0.98       100\n",
            "            IDDPM       0.75      0.72      0.73       100\n",
            "              LDM       0.87      0.86      0.86       100\n",
            "             PNDM       0.90      0.90      0.90       100\n",
            "           ProGAN       0.94      0.95      0.95       100\n",
            "     ProjectedGAN       0.65      0.74      0.69       100\n",
            "         StyleGAN       0.98      0.95      0.96       100\n",
            "\n",
            "         accuracy                           0.84      1001\n",
            "        macro avg       0.84      0.84      0.84      1001\n",
            "     weighted avg       0.84      0.84      0.84      1001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --config configs/dino.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z5MXZ9D6J-hW",
        "outputId": "9608ee16-c839-4c87-a659-8e3cb40cb748"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17 12:24:02.259018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765974242.278830    5531 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765974242.284868    5531 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765974242.300342    5531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765974242.300367    5531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765974242.300371    5531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765974242.300376    5531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 12:24:02.305097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Configuration loaded from configs/dino.json\n",
            "Using device: cuda\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: e0225f94725c855bf1dde0275f62942a2d1b4985\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Invalid choice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkipyo39\u001b[0m (\u001b[33mkipyo39-sungkyunkwan-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run 292xwi6b (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Intelligence_Vision/wandb/run-20251217_122442-292xwi6b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdino_finetune\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kipyo39-sungkyunkwan-university/Gen-Image-Classifier\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kipyo39-sungkyunkwan-university/Gen-Image-Classifier/runs/292xwi6b\u001b[0m\n",
            "Scanning train dataset in ./data/train...\n",
            "  [ADM] found 799 images\n",
            "  [DDPM] found 800 images\n",
            "  [Diff-ProjectedGAN] found 800 images\n",
            "  [Diff-StyleGAN2] found 800 images\n",
            "  [IDDPM] found 800 images\n",
            "  [LDM] found 800 images\n",
            "  [PNDM] found 800 images\n",
            "  [ProGAN] found 800 images\n",
            "  [ProjectedGAN] found 800 images\n",
            "  [StyleGAN] found 800 images\n",
            "Total train images: 7999\n",
            "\n",
            "Scanning val dataset in ./data/val...\n",
            "  [ADM] found 100 images\n",
            "  [DDPM] found 100 images\n",
            "  [Diff-ProjectedGAN] found 100 images\n",
            "  [Diff-StyleGAN2] found 100 images\n",
            "  [IDDPM] found 100 images\n",
            "  [LDM] found 100 images\n",
            "  [PNDM] found 100 images\n",
            "  [ProGAN] found 100 images\n",
            "  [ProjectedGAN] found 100 images\n",
            "  [StyleGAN] found 100 images\n",
            "Total val images: 1000\n",
            "\n",
            "Scanning test dataset in ./data/test...\n",
            "  [ADM] found 101 images\n",
            "  [DDPM] found 100 images\n",
            "  [Diff-ProjectedGAN] found 100 images\n",
            "  [Diff-StyleGAN2] found 100 images\n",
            "  [IDDPM] found 100 images\n",
            "  [LDM] found 100 images\n",
            "  [PNDM] found 100 images\n",
            "  [ProGAN] found 100 images\n",
            "  [ProjectedGAN] found 100 images\n",
            "  [StyleGAN] found 100 images\n",
            "Total test images: 1001\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Loading DINO model: facebook/dinov2-base\n",
            "config.json: 100% 548/548 [00:00<00:00, 1.36MB/s]\n",
            "model.safetensors: 100% 346M/346M [00:05<00:00, 67.1MB/s]\n",
            "Training parameters: Classifier\n",
            "Freezing parameters: DINO Encoder\n",
            "\n",
            "Optimizer: AdamW, Scheduler: CosineAnnealingLR\n",
            "\n",
            "--- Starting Training ---\n",
            "\n",
            "Epoch 1/20\n",
            "Training: 100% 250/250 [01:55<00:00,  2.17it/s, loss=1.9945, acc=24.43%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 1: Train Loss=2.0858, Acc=24.43% | Val Loss=1.7500, Acc=35.30%\n",
            "--> New best model saved with Val Acc: 35.30%\n",
            "\n",
            "Epoch 2/20\n",
            "Training: 100% 250/250 [01:55<00:00,  2.17it/s, loss=1.8435, acc=37.79%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.16it/s]\n",
            "Epoch 2: Train Loss=1.7143, Acc=37.79% | Val Loss=1.4997, Acc=44.40%\n",
            "--> New best model saved with Val Acc: 44.40%\n",
            "\n",
            "Epoch 3/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=1.4927, acc=44.19%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 3: Train Loss=1.5141, Acc=44.19% | Val Loss=1.3617, Acc=47.60%\n",
            "--> New best model saved with Val Acc: 47.60%\n",
            "\n",
            "Epoch 4/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=1.3461, acc=49.99%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.16it/s]\n",
            "Epoch 4: Train Loss=1.3551, Acc=49.99% | Val Loss=1.2823, Acc=50.60%\n",
            "--> New best model saved with Val Acc: 50.60%\n",
            "\n",
            "Epoch 5/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=1.1670, acc=55.02%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 5: Train Loss=1.2372, Acc=55.02% | Val Loss=1.2305, Acc=52.30%\n",
            "--> New best model saved with Val Acc: 52.30%\n",
            "\n",
            "Epoch 6/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=1.0443, acc=58.92%]\n",
            "Validating: 100% 32/32 [00:15<00:00,  2.13it/s]\n",
            "Epoch 6: Train Loss=1.1352, Acc=58.92% | Val Loss=1.1918, Acc=54.80%\n",
            "--> New best model saved with Val Acc: 54.80%\n",
            "\n",
            "Epoch 7/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=1.2658, acc=61.96%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 7: Train Loss=1.0489, Acc=61.96% | Val Loss=1.1292, Acc=55.10%\n",
            "--> New best model saved with Val Acc: 55.10%\n",
            "\n",
            "Epoch 8/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=1.1379, acc=65.02%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.17it/s]\n",
            "Epoch 8: Train Loss=0.9721, Acc=65.02% | Val Loss=1.0969, Acc=56.20%\n",
            "--> New best model saved with Val Acc: 56.20%\n",
            "\n",
            "Epoch 9/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=1.0006, acc=68.07%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.18it/s]\n",
            "Epoch 9: Train Loss=0.9015, Acc=68.07% | Val Loss=1.0849, Acc=56.20%\n",
            "\n",
            "Epoch 10/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=0.7916, acc=70.10%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.16it/s]\n",
            "Epoch 10: Train Loss=0.8304, Acc=70.10% | Val Loss=1.0768, Acc=57.00%\n",
            "--> New best model saved with Val Acc: 57.00%\n",
            "\n",
            "Epoch 11/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.17it/s, loss=0.8421, acc=72.27%]\n",
            "Validating: 100% 32/32 [00:15<00:00,  2.13it/s]\n",
            "Epoch 11: Train Loss=0.7734, Acc=72.27% | Val Loss=1.0574, Acc=59.30%\n",
            "--> New best model saved with Val Acc: 59.30%\n",
            "\n",
            "Epoch 12/20\n",
            "Training: 100% 250/250 [01:55<00:00,  2.17it/s, loss=0.6990, acc=74.98%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.16it/s]\n",
            "Epoch 12: Train Loss=0.7218, Acc=74.98% | Val Loss=1.0736, Acc=57.20%\n",
            "\n",
            "Epoch 13/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=0.9205, acc=76.68%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.13it/s]\n",
            "Epoch 13: Train Loss=0.6699, Acc=76.68% | Val Loss=1.0578, Acc=58.20%\n",
            "\n",
            "Epoch 14/20\n",
            "Training: 100% 250/250 [01:55<00:00,  2.17it/s, loss=0.6557, acc=78.10%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.15it/s]\n",
            "Epoch 14: Train Loss=0.6282, Acc=78.10% | Val Loss=1.0408, Acc=59.80%\n",
            "--> New best model saved with Val Acc: 59.80%\n",
            "\n",
            "Epoch 15/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=0.5682, acc=80.00%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.15it/s]\n",
            "Epoch 15: Train Loss=0.5852, Acc=80.00% | Val Loss=1.0369, Acc=60.60%\n",
            "--> New best model saved with Val Acc: 60.60%\n",
            "\n",
            "Epoch 16/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=0.4969, acc=82.06%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 16: Train Loss=0.5368, Acc=82.06% | Val Loss=1.0780, Acc=60.20%\n",
            "\n",
            "Epoch 17/20\n",
            "Training: 100% 250/250 [01:55<00:00,  2.17it/s, loss=0.4086, acc=82.82%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 17: Train Loss=0.5080, Acc=82.82% | Val Loss=1.0601, Acc=60.20%\n",
            "\n",
            "Epoch 18/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=0.4819, acc=84.59%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 18: Train Loss=0.4665, Acc=84.59% | Val Loss=1.0491, Acc=60.70%\n",
            "--> New best model saved with Val Acc: 60.70%\n",
            "\n",
            "Epoch 19/20\n",
            "Training: 100% 250/250 [01:55<00:00,  2.17it/s, loss=0.5072, acc=85.61%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.14it/s]\n",
            "Epoch 19: Train Loss=0.4405, Acc=85.61% | Val Loss=1.0780, Acc=59.60%\n",
            "\n",
            "Epoch 20/20\n",
            "Training: 100% 250/250 [01:54<00:00,  2.18it/s, loss=0.4621, acc=86.20%]\n",
            "Validating: 100% 32/32 [00:14<00:00,  2.16it/s]\n",
            "Epoch 20: Train Loss=0.4180, Acc=86.20% | Val Loss=1.0784, Acc=61.50%\n",
            "--> New best model saved with Val Acc: 61.50%\n",
            "\n",
            "--- Training Finished. Best Val Acc: 61.50% ---\n",
            "\n",
            "--- Starting Testing with the Best Model ---\n",
            "Model loaded from outputs/dino/best_model.pth\n",
            "Testing: 100% 32/32 [00:15<00:00,  2.07it/s]\n",
            "\n",
            "Test Results: Loss=1.1614, Accuracy=59.34%\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "              ADM       0.55      0.59      0.57       101\n",
            "             DDPM       0.39      0.39      0.39       100\n",
            "Diff-ProjectedGAN       0.68      0.50      0.58       100\n",
            "   Diff-StyleGAN2       0.76      0.71      0.74       100\n",
            "            IDDPM       0.37      0.34      0.36       100\n",
            "              LDM       0.33      0.42      0.37       100\n",
            "             PNDM       0.58      0.41      0.48       100\n",
            "           ProGAN       0.82      0.94      0.87       100\n",
            "     ProjectedGAN       0.58      0.67      0.62       100\n",
            "         StyleGAN       0.90      0.96      0.93       100\n",
            "\n",
            "         accuracy                           0.59      1001\n",
            "        macro avg       0.60      0.59      0.59      1001\n",
            "     weighted avg       0.60      0.59      0.59      1001\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading history steps 20-20, summary, console lines 169-189 (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading history steps 20-20, summary, console lines 169-189 (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading history steps 20-20, summary, console lines 169-189 (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr █████▇▇▇▇▆▆▅▅▅▄▃▃▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_acc ▁▃▃▄▄▅▅▆▆▆▆▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss █▆▆▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_acc ▁▃▄▅▆▆▆▇▇▇▇▇▇█████▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss █▆▄▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: best_val_accuracy 61.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                lr 7e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test_accuracy 59.34066\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         test_loss 1.16136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_acc 86.19827\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_loss 0.41802\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val_acc 61.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          val_loss 1.0784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdino_finetune\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kipyo39-sungkyunkwan-university/Gen-Image-Classifier/runs/292xwi6b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kipyo39-sungkyunkwan-university/Gen-Image-Classifier\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251217_122442-292xwi6b/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --config configs/mfm_clip.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IWI1v_VmM5MT",
        "outputId": "05e4fd14-7a2a-41ad-9575-80f63fc6cd5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17 13:31:46.903629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765978306.923926    1207 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765978306.929837    1207 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765978306.945529    1207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978306.945556    1207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978306.945560    1207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978306.945565    1207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 13:31:46.950242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Configuration loaded from configs/mfm_clip.json\n",
            "Using device: cuda\n",
            "Scanning train dataset in ./data/train...\n",
            "  [ADM] found 799 images\n",
            "  [DDPM] found 800 images\n",
            "  [Diff-ProjectedGAN] found 800 images\n",
            "  [Diff-StyleGAN2] found 800 images\n",
            "  [IDDPM] found 800 images\n",
            "  [LDM] found 800 images\n",
            "  [PNDM] found 800 images\n",
            "  [ProGAN] found 800 images\n",
            "  [ProjectedGAN] found 800 images\n",
            "  [StyleGAN] found 800 images\n",
            "Total train images: 7999\n",
            "\n",
            "Scanning val dataset in ./data/val...\n",
            "  [ADM] found 100 images\n",
            "  [DDPM] found 100 images\n",
            "  [Diff-ProjectedGAN] found 100 images\n",
            "  [Diff-StyleGAN2] found 100 images\n",
            "  [IDDPM] found 100 images\n",
            "  [LDM] found 100 images\n",
            "  [PNDM] found 100 images\n",
            "  [ProGAN] found 100 images\n",
            "  [ProjectedGAN] found 100 images\n",
            "  [StyleGAN] found 100 images\n",
            "Total val images: 1000\n",
            "\n",
            "Scanning test dataset in ./data/test...\n",
            "  [ADM] found 101 images\n",
            "  [DDPM] found 100 images\n",
            "  [Diff-ProjectedGAN] found 100 images\n",
            "  [Diff-StyleGAN2] found 100 images\n",
            "  [IDDPM] found 100 images\n",
            "  [LDM] found 100 images\n",
            "  [PNDM] found 100 images\n",
            "  [ProGAN] found 100 images\n",
            "  [ProjectedGAN] found 100 images\n",
            "  [StyleGAN] found 100 images\n",
            "Total test images: 1001\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Loading CLIP model: openai/clip-vit-base-patch32\n",
            "config.json: 4.19kB [00:00, 18.8MB/s]\n",
            "pytorch_model.bin: 100% 605M/605M [00:03<00:00, 176MB/s]\n",
            "Training parameters: Classifier\n",
            "Training parameters: MFM Encoder\n",
            "Freezing parameters: CLIP Encoder\n",
            "\n",
            "Optimizer: AdamW, Scheduler: CosineAnnealingLR\n",
            "\n",
            "--- Starting Training ---\n",
            "\n",
            "Epoch 1/20\n",
            "Training:   0% 0/250 [00:00<?, ?it/s]\n",
            "model.safetensors:   0% 0.00/605M [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   0% 1.70M/605M [00:00<03:52, 2.59MB/s]\u001b[A\n",
            "model.safetensors:   6% 34.4M/605M [00:01<00:23, 24.4MB/s]\u001b[A\n",
            "model.safetensors:   7% 42.8M/605M [00:02<00:26, 21.3MB/s]\u001b[A\n",
            "model.safetensors:  18% 110M/605M [00:02<00:07, 66.7MB/s] \u001b[A\n",
            "model.safetensors:  25% 151M/605M [00:04<00:11, 40.2MB/s]\u001b[A\n",
            "model.safetensors:  36% 218M/605M [00:04<00:06, 60.4MB/s]\u001b[A\n",
            "model.safetensors:  47% 286M/605M [00:04<00:03, 82.5MB/s]\u001b[A\n",
            "model.safetensors:  58% 353M/605M [00:05<00:02, 113MB/s] \u001b[A\n",
            "model.safetensors:  69% 420M/605M [00:05<00:01, 150MB/s]\u001b[A\n",
            "model.safetensors:  80% 487M/605M [00:05<00:00, 162MB/s]\u001b[A\n",
            "model.safetensors: 100% 605M/605M [00:05<00:00, 103MB/s]\n",
            "Training:   4% 10/250 [00:15<06:23,  1.60s/it, loss=2.3637, acc=11.56%]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Intelligence_Vision/main.py\", line 448, in <module>\n",
            "    main()\n",
            "  File \"/content/Intelligence_Vision/main.py\", line 444, in main\n",
            "    trainer.train()\n",
            "  File \"/content/Intelligence_Vision/main.py\", line 316, in train\n",
            "    train_loss, train_acc = self._train_epoch()\n",
            "                            ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Intelligence_Vision/main.py\", line 348, in _train_epoch\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 625, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 841, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python clip_prompt.py --config configs/clip_prompt.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOFPc-glM7E8",
        "outputId": "be46203a-bcdf-4414-bad7-f71ebfad87aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17 13:33:02.677488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765978382.710245    1661 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765978382.722294    1661 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765978382.747980    1661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978382.748031    1661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978382.748040    1661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978382.748048    1661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 13:33:02.754202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Configuration loaded from configs/clip_prompt.json\n",
            "\n",
            "데이터셋 로드 중...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "데이터셋 크기 - Train: 7999, Val: 1000, Test: 1001\n",
            "\n",
            "CLIP Deep Prompt Tuning 모델 초기화 중...\n",
            "CLIP 모델 로드: openai/clip-vit-base-patch32\n",
            "Deep Prompt Tuning이 활성화되었습니다.\n",
            " - Vision Prompts 생성: 12 layers, 20 tokens, 768 dim\n",
            "\n",
            "옵티마이저 설정:\n",
            " - Deep Prompts (Vision)와 Classifier 파라미터만 학습합니다.\n",
            "Epoch 1/20 [Train]: 100% 250/250 [01:10<00:00,  3.53it/s, loss=1.1292, acc=56.54%]\n",
            "Epoch 1: Val Loss: 0.4628, Val Acc: 76.60%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 2/20 [Train]: 100% 250/250 [01:11<00:00,  3.52it/s, loss=0.4124, acc=79.38%]\n",
            "Epoch 2: Val Loss: 0.3505, Val Acc: 82.30%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 3/20 [Train]: 100% 250/250 [01:11<00:00,  3.50it/s, loss=0.3259, acc=83.70%]\n",
            "Epoch 3: Val Loss: 0.3683, Val Acc: 82.20%\n",
            "Epoch 4/20 [Train]: 100% 250/250 [01:11<00:00,  3.50it/s, loss=0.2791, acc=87.01%]\n",
            "Epoch 4: Val Loss: 0.3238, Val Acc: 86.60%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 5/20 [Train]: 100% 250/250 [01:11<00:00,  3.50it/s, loss=0.2315, acc=89.40%]\n",
            "Epoch 5: Val Loss: 0.2439, Val Acc: 88.30%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 6/20 [Train]: 100% 250/250 [01:12<00:00,  3.47it/s, loss=0.1895, acc=91.54%]\n",
            "Epoch 6: Val Loss: 0.2336, Val Acc: 90.10%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 7/20 [Train]: 100% 250/250 [01:11<00:00,  3.51it/s, loss=0.1626, acc=92.96%]\n",
            "Epoch 7: Val Loss: 0.2069, Val Acc: 89.80%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 8/20 [Train]: 100% 250/250 [01:11<00:00,  3.48it/s, loss=0.1360, acc=94.14%]\n",
            "Epoch 8: Val Loss: 0.1901, Val Acc: 92.50%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 9/20 [Train]: 100% 250/250 [01:11<00:00,  3.51it/s, loss=0.1205, acc=95.24%]\n",
            "Epoch 9: Val Loss: 0.1894, Val Acc: 92.00%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 10/20 [Train]: 100% 250/250 [01:11<00:00,  3.52it/s, loss=0.1149, acc=95.49%]\n",
            "Epoch 10: Val Loss: 0.1888, Val Acc: 92.50%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 11/20 [Train]: 100% 250/250 [01:11<00:00,  3.49it/s, loss=0.1117, acc=95.81%]\n",
            "Epoch 11: Val Loss: 0.1888, Val Acc: 92.50%\n",
            "Epoch 12/20 [Train]: 100% 250/250 [01:11<00:00,  3.51it/s, loss=0.1129, acc=95.61%]\n",
            "Epoch 12: Val Loss: 0.1872, Val Acc: 92.40%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 13/20 [Train]: 100% 250/250 [01:11<00:00,  3.50it/s, loss=0.1128, acc=95.39%]\n",
            "Epoch 13: Val Loss: 0.1880, Val Acc: 92.50%\n",
            "Epoch 14/20 [Train]: 100% 250/250 [01:11<00:00,  3.51it/s, loss=0.1173, acc=95.34%]\n",
            "Epoch 14: Val Loss: 0.1836, Val Acc: 93.20%\n",
            "--> 최고 모델 저장됨: output_clip_prompt/best_model_prompt.pth\n",
            "Epoch 15/20 [Train]: 100% 250/250 [01:11<00:00,  3.50it/s, loss=0.1251, acc=95.00%]\n",
            "Epoch 15: Val Loss: 0.2143, Val Acc: 90.50%\n",
            "Epoch 16/20 [Train]: 100% 250/250 [01:11<00:00,  3.49it/s, loss=0.1204, acc=95.00%]\n",
            "Epoch 16: Val Loss: 0.2202, Val Acc: 91.10%\n",
            "Epoch 17/20 [Train]: 100% 250/250 [01:11<00:00,  3.51it/s, loss=0.1257, acc=94.74%]\n",
            "Epoch 17: Val Loss: 0.2275, Val Acc: 90.60%\n",
            "Epoch 18/20 [Train]: 100% 250/250 [01:11<00:00,  3.49it/s, loss=0.1154, acc=95.12%]\n",
            "Epoch 18: Val Loss: 0.1897, Val Acc: 92.80%\n",
            "Epoch 19/20 [Train]: 100% 250/250 [01:11<00:00,  3.50it/s, loss=0.1310, acc=94.71%]\n",
            "Epoch 19: Val Loss: 0.2183, Val Acc: 91.10%\n",
            "Epoch 20/20 [Train]: 100% 250/250 [01:11<00:00,  3.50it/s, loss=0.1090, acc=95.57%]\n",
            "Epoch 20: Val Loss: 0.2463, Val Acc: 90.60%\n",
            "\n",
            "최고 모델 테스트: output_clip_prompt/best_model_prompt.pth\n",
            "Testing: 100% 32/32 [00:06<00:00,  4.93it/s]\n",
            "\n",
            "테스트 결과:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "              ADM       0.73      0.67      0.70       101\n",
            "             DDPM       1.00      0.99      0.99       100\n",
            "Diff-ProjectedGAN       0.91      0.89      0.90       100\n",
            "   Diff-StyleGAN2       0.99      1.00      1.00       100\n",
            "            IDDPM       0.69      0.74      0.71       100\n",
            "              LDM       0.98      0.98      0.98       100\n",
            "             PNDM       1.00      0.99      0.99       100\n",
            "           ProGAN       0.97      1.00      0.99       100\n",
            "     ProjectedGAN       0.90      0.89      0.89       100\n",
            "         StyleGAN       0.98      1.00      0.99       100\n",
            "\n",
            "         accuracy                           0.92      1001\n",
            "        macro avg       0.92      0.92      0.91      1001\n",
            "     weighted avg       0.91      0.92      0.91      1001\n",
            "\n",
            "전체 정확도: 91.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python mfm_clip_prompt.py --config configs/mfm_clip_prompt.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eANUzqC-NhMs",
        "outputId": "f9429899-fac4-4d65-e6a3-a2bb3efebaeb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17 13:32:29.517635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765978349.537749    1493 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765978349.543774    1493 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765978349.559459    1493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978349.559481    1493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978349.559485    1493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765978349.559490    1493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 13:32:29.564107: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Configuration loaded from configs/mfm_clip_prompt.json\n",
            "\n",
            "데이터셋 로드 중...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "데이터셋 크기 - Train: 7999, Val: 1000, Test: 1001\n",
            "\n",
            "통합 모델 초기화 중...\n",
            "CLIP 모델 로드: openai/clip-vit-base-patch32\n",
            "Deep Prompt Tuning이 활성화되었습니다.\n",
            " - MFM Prompts 생성: 12 layers, 20 tokens, 768 dim\n",
            " - Vision Prompts 생성: 12 layers, 20 tokens, 768 dim\n",
            "\n",
            "옵티마이저 설정:\n",
            " - Deep Prompts (MFM & Vision)와 Classifier 파라미터만 학습합니다.\n",
            "Epoch 1/20 [Train]:   3% 7/250 [00:10<05:55,  1.46s/it, loss=2.3448, acc=8.48%]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Intelligence_Vision/mfm_clip_prompt.py\", line 417, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/Intelligence_Vision/mfm_clip_prompt.py\", line 336, in train\n",
            "    total_loss += loss.item() * images.size(0)\n",
            "                  ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}